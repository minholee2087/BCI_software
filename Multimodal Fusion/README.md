# Adaptive Multimodal Bottleneck Transformer (AMBT)


**Adaptive Multimodal EEG‚ÄìAudio‚ÄìVideo Fusion Transformer for Emotion Recognition**

This repository provides the official implementation of **AMBT (Adaptive Multimodal Bottleneck Transformer)** ‚Äî a **parameter-efficient multimodal Transformer** for **brain‚Äìcomputer interface (BCI)** emotion recognition, designed to fuse **EEG**, **speech audio**, and **facial video** via **adaptive bottleneck-token interaction** inside intermediate Transformer layers.

> üìå This software is developed as a research and commercialization deliverable of the grant project:
> **‚ÄúDevelopment of Brain‚ÄìComputer Interface (BCI) SW/HW Solutions‚Äù** (Commercialization Projects Funding)
---

## ‚ú® Overview

Emotion recognition in conversational settings requires robust multimodal understanding. Facial and speech expressions are strong external cues, while **EEG provides complementary neural evidence**, especially when external signals are ambiguous or missing.

However, integrating heterogeneous modalities remains challenging due to:

* different sampling rates (EEG vs audio vs video),
* different feature spaces,
* different backbone architectures and tokenization methods.

**AMBT** addresses these challenges by introducing **Cross-Modal Adaptation Modules (CMAMs)** that enable **controlled cross-modal interaction** through **bottleneck tokens**, inserted into intermediate layers of modality-specific Transformer encoders.

---

## üß† Key Ideas

AMBT performs fusion using **adapter-style bottleneck interaction**:

- **Stable fusion** through bottleneck tokens (reduces premature collapse)
- **Preserves unimodal capacity** while enriching representations with cross-modal context
- **Architecture-agnostic integration** across heterogeneous Transformer encoders
- **Parameter-efficient training** (**< 1%** additional trainable parameters)

---

## üèóÔ∏è Method

### Adaptive Multimodal Bottleneck Transformer (AMBT)

CMAM injects multimodal interaction into intermediate Transformer layers using bottleneck tokens shared across modalities.

<p align="center">
  <img src="ambt_overview.png" alt="AMBT Overview" width="820"/>
</p>

> **Figure 1.** Overview of AMBT: unimodal encoders + CMAM adapters with bottleneck tokens enabling cross-modal exchange.

---

### Bottleneck Token Interaction

<p align="center">
  <img src="cmam_block.png" alt="CMAM Block" width="720"/>
</p>

> **Figure 2.** CMAM block: bottleneck tokens interact with modality-specific tokens and propagate fused context.

---

## üìä Results

Evaluated on three benchmark datasets:

| Dataset | Modalities | Accuracy |
|--------|------------|----------|
| **EAV** | EEG + Audio + Video | **85.1%** |
| **CREMA-D** | Audio + Video | **90.9%** |
| **DEAP** | EEG + Video | **98.7%** |

---

## üî• Software Features

* Independent Transformer encoders for **EEG**, **audio**, and **video**
* **CMAM** modules for controlled bottleneck-token fusion
* Parameter-efficient multimodal adaptation (**< 1% trainable params**)
* Unified latent space for fused multimodal representations
* Supported modalities and formats:

  * EEG (DEAP / EAV format)
  * Audio spectrograms (CREMA-D / EAV)
  * Facial video (CREMA-D / DEAP / EAV)
* Modular research codebase:

  * `src/models/` ‚Äì unimodal + fusion models
  * `src/datasets/` ‚Äì dataset loaders
  * `src/training/` ‚Äì training & evaluation scripts
  * `src/utils/` ‚Äì config, logging, metrics, checkpointing, seeding

---

## ‚öôÔ∏è Installation

### Windows

Run:

```bash
install.bat
```

This will:

* Verify Python **3.8+** is installed
* Create a virtual environment (`venv/`)
* Install dependencies from `requirements.txt`

---

## ‚öôÔ∏è Installation

### Windows (Recommended)

Run:

```bash
install.bat
```

This will:

* verify Python **3.8+** is installed
* create a virtual environment (`venv/`)
* install dependencies from `requirements.txt`

---

## üöÄ Usage

Run the training pipeline:

```bash
run.bat
```

This script will:

* check that the virtual environment (`venv/`) exists
* if missing, prompt you to run `install.bat` first
* activate the virtual environment
* launch training using the **DEAP** configuration:

```bat
@echo off
echo ========================================
echo Multimodal Emotion Recognition - Run
echo ========================================
echo.

REM Activate venv
if not exist venv (
    echo Virtual environment not found.
    echo Please run install.bat first.
    pause
    exit /b 1
)

call venv\Scripts\activate.bat

REM Run training
python src/training/train.py --config configs/deap.yaml

pause
```

---


## üì¶ Datasets

### EAV (EEG-Audio-Video Dataset)

* Modalities: **EEG + Audio + Video**
* EEG recordings synchronized with audio and video
* Preprocessing: `datasets/EAV/`
* Config: `configs/eav.yaml`

### CREMA-D

* Modalities: **Audio + Video**
* Emotion-labeled speech clips from multiple actors
* Preprocessing: `datasets/Crema-D/`
* Config: `configs/cremad.yaml`

### DEAP

* Modalities: **EEG + Video**
* EEG-based emotion recognition benchmark dataset
* Preprocessing:  `datasets/DEAP/`
* Config: `configs/deap.yaml`


---

## üìÅ Repository Structure

```text
.
‚îú‚îÄ‚îÄ install.bat
‚îú‚îÄ‚îÄ run.bat
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ configs
‚îÇ   ‚îú‚îÄ‚îÄ cremad.yaml
‚îÇ   ‚îú‚îÄ‚îÄ deap.yaml
‚îÇ   ‚îú‚îÄ‚îÄ default.yaml
‚îÇ   ‚îî‚îÄ‚îÄ eav.yaml
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ datasets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Crema-D
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audio_load.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ video_load.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DEAP
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataload_eeg.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dataload_vid.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ EAV
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Dataload_audio.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Dataload_eeg.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Dataload_vision.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ EAV_datasplit.py
‚îÇ   ‚îú‚îÄ‚îÄ models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AMBT_fusion
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AMBT_concat.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AMBT_mean.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Transformer_Audio_concat.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Transformer_Audio_mean.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Transformer_EEG_concat.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Transformer_EEG_mean.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Transformer_Video_concat.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Transformer_Video_mean.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ unimodal
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ Audio_Transformer
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ Audio_Transformer_main.py
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ Audio_Transformer_model.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ EEG_Transformer
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ eeg_eegnet_transformer.py
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ eeg_shallow_transformer.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ Video_Transformer
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ cls_tokens.pth
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ position_embeddings.pth
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ Video_Transformer_main.py
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ Video_Transformer_model.py
‚îÇ   ‚îú‚îÄ‚îÄ training
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train.py
‚îÇ   ‚îî‚îÄ‚îÄ utils
‚îÇ       ‚îú‚îÄ‚îÄ checkpoint.py
‚îÇ       ‚îú‚îÄ‚îÄ config.py
‚îÇ       ‚îú‚îÄ‚îÄ device.py
‚îÇ       ‚îú‚îÄ‚îÄ logging.py
‚îÇ       ‚îî‚îÄ‚îÄ metrics.py
‚îî‚îÄ‚îÄ main.py
