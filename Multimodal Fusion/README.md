# Adaptive Multimodal Bottleneck Transformer (AMBT)


> **AMBT** is a parameter-efficient multimodal Transformer for **emotion recognition** that fuses **EEG**, **audio**, and **facial video** via **adaptive bottleneck-token interaction** inside intermediate Transformer layers.

---

## âœ¨ Overview

Facial and speech expressions are strong cues for emotion recognition, while EEG provides complementary neural information when external signals are ambiguous or missing. However, integrating heterogeneous modalities remains challenging due to different sampling rates, feature spaces, and backbone architectures.

**Adaptive Multimodal Bottleneck Transformer (AMBT)** addresses this by introducing **Cross-Modal Adaptation Modules (CMAMs)** that enable controlled and stable cross-modal interaction through bottleneck tokens.

---

## ðŸ§  Key Ideas

AMBT performs fusion using **adapter-style bottleneck interaction**:

- **Stable fusion** through bottleneck tokens (reduces premature collapse)
- **Preserves unimodal capacity** while enriching representations with cross-modal context
- **Architecture-agnostic integration** across heterogeneous Transformer encoders
- **Parameter-efficient training** (**< 1%** additional trainable parameters)

---

## ðŸ—ï¸ Method

### Cross-Modal Adaptation Module (CMAM)

CMAM injects multimodal interaction into intermediate Transformer layers using bottleneck tokens shared across modalities.

<p align="center">
  <img src="ambt.png" alt="AMBT Overview" width="820"/>
</p>

> **Figure 1.** Overview of AMBT: unimodal encoders + CMAM adapters with bottleneck tokens enabling cross-modal exchange.

---

### Bottleneck Token Interaction

<p align="center">
  <img src="cmam.png" alt="CMAM Block" width="820"/>
</p>

> **Figure 2.** CMAM block: bottleneck tokens interact with modality-specific tokens and propagate fused context.

---

## ðŸ“Š Results

Evaluated on three benchmark datasets:

| Dataset | Modalities | Accuracy |
|--------|------------|----------|
| **EAV** | EEG + Audio + Video | **85.1%** |
| **CREMA-D** | Audio + Video | **90.9%** |
| **DEAP** | EEG + Video | **98.7%** |

---

## ðŸ”¥ Features

- Independent Transformer encoders for EEG, audio, and video
- **CMAM** modules for controlled bottleneck-token fusion
- Parameter-efficient multimodal adaptation (**< 1%** trainable params)
- Unified latent space for fused multimodal representations
- Supports:
  - EEG (DEAP / EAV format)
  - Audio spectrograms (CREMA-D / EAV)
  - Facial video (CREMA-D / DEAP / EAV)
- Modular codebase:
  - `src/models/` â€“ unimodal + fusion models
  - `src/datasets/` â€“ dataset loaders
  - `src/training/` â€“ training & evaluation scripts
  - `src/utils/` â€“ config, logging, metrics, checkpointing, seeding

---

## ðŸ“ Repository Structure

```text
.
â”œâ”€â”€ install.bat
â”œâ”€â”€ run.bat
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ configs
â”‚   â”œâ”€â”€ cremad.yaml
â”‚   â”œâ”€â”€ deap.yaml
â”‚   â”œâ”€â”€ default.yaml
â”‚   â””â”€â”€ eav.yaml
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ datasets
â”‚   â”‚   â”œâ”€â”€ Crema-D
â”‚   â”‚   â”‚   â”œâ”€â”€ audio_load.py
â”‚   â”‚   â”‚   â””â”€â”€ video_load.py
â”‚   â”‚   â”œâ”€â”€ DEAP
â”‚   â”‚   â”‚   â”œâ”€â”€ dataload_eeg.py
â”‚   â”‚   â”‚   â””â”€â”€ dataload_vid.py
â”‚   â”‚   â””â”€â”€ EAV
â”‚   â”‚       â”œâ”€â”€ Dataload_audio.py
â”‚   â”‚       â”œâ”€â”€ Dataload_eeg.py
â”‚   â”‚       â”œâ”€â”€ Dataload_vision.py
â”‚   â”‚       â””â”€â”€ EAV_datasplit.py
â”‚   â”œâ”€â”€ models
â”‚   â”‚   â”œâ”€â”€ AMBT_fusion
â”‚   â”‚   â”‚   â”œâ”€â”€ AMBT_concat.py
â”‚   â”‚   â”‚   â”œâ”€â”€ AMBT_mean.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Transformer_Audio_concat.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Transformer_Audio_mean.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Transformer_EEG_concat.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Transformer_EEG_mean.py
â”‚   â”‚   â”‚   â”œâ”€â”€ Transformer_Video_concat.py
â”‚   â”‚   â”‚   â””â”€â”€ Transformer_Video_mean.py
â”‚   â”‚   â””â”€â”€ unimodal
â”‚   â”‚       â”œâ”€â”€ Audio_Transformer
â”‚   â”‚       â”‚   â”œâ”€â”€ Audio_Transformer_main.py
â”‚   â”‚       â”‚   â””â”€â”€ Audio_Transformer_model.py
â”‚   â”‚       â”œâ”€â”€ EEG_Transformer
â”‚   â”‚       â”‚   â”œâ”€â”€ eeg_eegnet_transformer.py
â”‚   â”‚       â”‚   â””â”€â”€ eeg_shallow_transformer.py
â”‚   â”‚       â””â”€â”€ Video_Transformer
â”‚   â”‚           â”œâ”€â”€ cls_tokens.pth
â”‚   â”‚           â”œâ”€â”€ position_embeddings.pth
â”‚   â”‚           â”œâ”€â”€ Video_Transformer_main.py
â”‚   â”‚           â””â”€â”€ Video_Transformer_model.py
â”‚   â”œâ”€â”€ training
â”‚   â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”‚   â””â”€â”€ train.py
â”‚   â””â”€â”€ utils
â”‚       â”œâ”€â”€ checkpoint.py
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ device.py
â”‚       â”œâ”€â”€ logging.py
â”‚       â””â”€â”€ metrics.py
â””â”€â”€ tests
    â””â”€â”€ test_forward.py
